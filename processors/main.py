"""Central orchestration of all processing steps."""

from __future__ import annotations

import json
import os
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List

import pandas as pd

try:  # pragma: no cover - runtime convenience
    from . import (
        aggregation,
        bikes,
        chantiers,
        comptage_velo,
        config,
        metrics,
        qualite_service,
        referentiel_troncons,
        reports,
        storage,
        traffic,
        validations,
        weather,
    )
    from .base import PipelineResult, QualityReport
    from .config import get_config
    from .storage import InputReader, OutputWriter
except ImportError:  # pragma: no cover - executed when run as script directly
    sys.path.append(str(Path(__file__).resolve().parent.parent))
    from processors import aggregation
    from processors import bikes
    from processors import chantiers
    from processors import comptage_velo
    from processors import config
    from processors import metrics
    from processors import qualite_service
    from processors import referentiel_troncons
    from processors import reports
    from processors import storage
    from processors import traffic
    from processors import validations
    from processors import weather
    from processors.base import PipelineResult, QualityReport
    from processors.config import get_config
    from processors.storage import InputReader, OutputWriter


API_PROCESSORS = {
    "bikes": bikes.process,
    "traffic": traffic.process,
    "weather": weather.process,
}

BATCH_PROCESSORS = {
    "comptage-velo-donnees-compteurs-cleaned.csv": ("comptage_velo", comptage_velo.process),
    "chantiers-a-paris-cleaned.csv": ("chantiers", chantiers.process),
    "indicateurs-de-qualite-de-service-sncf-et-ratp.csv": ("qualite_service", qualite_service.process),
    "referentiel-geographique-pour-les-donnees-trafic-issues-des-capteurs-permanents.csv": ("referentiel_troncons", referentiel_troncons.process),
    "validations-reseau-surface-nombre-validations-par-jour-2eme-trimestre.csv": ("validations", validations.process),
}


@dataclass(slots=True)
class DailyProcessingOutput:
    api_results: Dict[str, PipelineResult]
    batch_results: Dict[str, PipelineResult]
    aggregates: Dict[str, pd.DataFrame]
    metrics_cityflow: Dict[str, any]
    correlations: Dict[str, pd.DataFrame]


def _merge_results(results: Iterable[PipelineResult]) -> PipelineResult:
    results_list = list(results)
    if not results_list:
        empty_quality = QualityReport(passed=True, messages=["info: no files found"])
        return PipelineResult(pd.DataFrame(), empty_quality, metadata={})

    frames = [res.dataframe for res in results_list if not res.dataframe.empty]
    dataframe = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()

    messages: List[str] = []
    passed = True
    metadata: List[Dict[str, object]] = []
    for res in results_list:
        messages.extend(res.quality_report.messages)
        passed = passed and res.quality_report.passed
        metadata.append(res.metadata)

    quality = QualityReport(passed=passed, messages=messages)
    combined_metadata = {"inputs": metadata}
    return PipelineResult(dataframe, quality, combined_metadata)


def _quality_summary(results: Dict[str, PipelineResult]) -> List[Dict[str, object]]:
    summary: List[Dict[str, object]] = []
    for name, result in results.items():
        summary.append(
            {
                "dataset": name,
                "passed": result.quality_report.passed,
                "messages": result.quality_report.messages,
                "rows": int(result.dataframe.shape[0]) if result.dataframe is not None else 0,
                "metadata": result.metadata,
            }
        )
    return summary


def _materialise_outputs(output_root: Path, date: str, outputs: DailyProcessingOutput, writer: OutputWriter = None) -> None:
    """
    Sauvegarde les outputs :
    - AgrÃ©gats : fichiers JSON locaux uniquement (pour compatibilitÃ©)
    - MÃ©triques, corrÃ©lations et rapports : base de donnÃ©es uniquement (MongoDB ou DynamoDB)
    """
    base_dir = output_root / date
    metrics_dir = base_dir / "metrics"
    metrics_dir.mkdir(parents=True, exist_ok=True)

    # Sauvegarder uniquement les agrÃ©gats en local (pour compatibilitÃ©)
    for name, dataframe in outputs.aggregates.items():
        if dataframe.empty:
            continue
        target = metrics_dir / f"{name}.json"
        dataframe.to_json(target, orient="records", indent=2, force_ascii=False)

    # GÃ©nÃ©rer les rapports (pour la base de donnÃ©es)
    report_payload = {
        "date": date,
        "api": _quality_summary(outputs.api_results),
        "batch": _quality_summary(outputs.batch_results),
        "aggregates": list(outputs.aggregates.keys()),
        "metrics_cityflow": list(outputs.metrics_cityflow.keys()),
        "correlations": list(outputs.correlations.keys())
    }
    
    # GÃ©nÃ©rer le rapport quotidien CityFlow
    rapport_complet = None
    df_comptage = outputs.batch_results.get("comptage_velo")
    if df_comptage and not df_comptage.dataframe.empty:
        rapport_complet = reports.generate_rapport_complet(
            df_comptage.dataframe,
            outputs.metrics_cityflow,
            date
        )
    
    # GÃ©nÃ©rer le rÃ©sumÃ© des mÃ©triques
    metrics_summary = reports.generate_metrics_summary(outputs.metrics_cityflow)
    
    # Sauvegarder dans la base de donnÃ©es (MongoDB ou DynamoDB)
    if writer is not None:
        print("   ğŸ“¤ Sauvegarde dans la base de donnÃ©es...")
        
        # PrÃ©parer les mÃ©triques pour la base de donnÃ©es
        metrics_for_db = {}
        for metric_name, metric_data in outputs.metrics_cityflow.items():
            if isinstance(metric_data, pd.DataFrame):
                if not metric_data.empty:
                    metrics_for_db[metric_name] = json.loads(metric_data.to_json(orient="records"))
            elif isinstance(metric_data, dict):
                if all(isinstance(v, pd.DataFrame) for v in metric_data.values()):
                    metrics_for_db[metric_name] = {
                        k: json.loads(df.to_json(orient="records")) if not df.empty else []
                        for k, df in metric_data.items()
                    }
                else:
                    metrics_for_db[metric_name] = metric_data
        
        # Sauvegarder les mÃ©triques
        if metrics_for_db:
            success_metrics = writer.write_metrics(date, metrics_for_db)
            if success_metrics:
                print("      âœ“ MÃ©triques sauvegardÃ©es dans la base")
            else:
                print("      âš ï¸  Ã‰chec sauvegarde mÃ©triques")
        
        # PrÃ©parer les corrÃ©lations pour la base de donnÃ©es
        correlations_for_db = {}
        for corr_name, corr_df in outputs.correlations.items():
            if not corr_df.empty:
                correlations_for_db[corr_name] = json.loads(corr_df.to_json(orient="records"))
        
        # Sauvegarder les corrÃ©lations
        if correlations_for_db:
            success_corr = writer.write_correlations(date, correlations_for_db)
            if success_corr:
                print("      âœ“ CorrÃ©lations sauvegardÃ©es dans la base")
            else:
                print("      âš ï¸  Ã‰chec sauvegarde corrÃ©lations")
        
        # Sauvegarder les 3 rapports dans la base de donnÃ©es
        all_reports = {
            "processing_report": report_payload,
            "metrics_summary": metrics_summary,
            "rapport_quotidien": rapport_complet
        }
        
        success_report = writer.write_reports(date, all_reports)
        if success_report:
            print("      âœ“ 3 rapports sauvegardÃ©s dans la base (processing_report, metrics_summary, rapport_quotidien)")
        else:
            print("      âš ï¸  Ã‰chec sauvegarde rapports")
    else:
        print("   âš ï¸  Aucun writer fourni : mÃ©triques, corrÃ©lations et rapports non sauvegardÃ©s")


def _process_api_sources(base_api: Path, date: str, reader: InputReader = None) -> Dict[str, PipelineResult]:
    results: Dict[str, PipelineResult] = {}
    cfg = get_config()
    
    for source, processor in API_PROCESSORS.items():
        source_dir = base_api / source / date
        
        # Si on est en mode AWS et qu'on a un reader, utiliser S3
        if cfg.is_aws and reader:
            # Lister les fichiers depuis S3
            s3_prefix = f"raw/api/{source}/{date}"
            files = reader.list_files(s3_prefix)
            
            if not files:
                continue
            
            print(f"   - Traitement API: {source}... ({len(files)} fichiers depuis S3)")
            
            # TÃ©lÃ©charger chaque fichier et le traiter
            runs = []
            for s3_file in sorted(files):
                # Extraire la clÃ© S3 depuis le path
                s3_key = str(s3_file).replace(f"/tmp/{cfg.s3_raw_bucket}/", "")
                local_path = Path(f"/tmp/{source}_{date}_{s3_file.name}")
                
                if reader.download_if_needed(s3_key, local_path):
                    runs.append(processor(local_path))
            
            if runs:
                results[source] = _merge_results(runs)
        else:
            # Mode local : utiliser le systÃ¨me de fichiers
            if not source_dir.exists():
                continue
            print(f"   - Traitement API: {source}...")
            runs = [processor(path) for path in sorted(source_dir.glob("*.json"))]
            results[source] = _merge_results(runs)
    
    return results


def _process_batch_sources(base_batch: Path, date: str, reader: InputReader = None) -> Dict[str, PipelineResult]:
    results: Dict[str, PipelineResult] = {}
    cfg = get_config()
    
    # Si on est en mode AWS et qu'on a un reader, utiliser S3
    if cfg.is_aws and reader:
        for filename, (key, processor) in BATCH_PROCESSORS.items():
            # Construire la clÃ© S3
            s3_key = f"raw/batch/{date}/{filename}"
            local_path = Path(f"/tmp/batch_{date}_{filename}")
            
            print(f"   - Traitement batch: {key} ({filename})...")
            
            # TÃ©lÃ©charger depuis S3
            if reader.download_if_needed(s3_key, local_path):
                results[key] = processor(local_path)
            else:
                print(f"      âš ï¸  Fichier {filename} non trouvÃ© dans S3")
    else:
        # Mode local : utiliser le systÃ¨me de fichiers
        if not base_batch.exists():
            return results
        for filename, (key, processor) in BATCH_PROCESSORS.items():
            file_path = base_batch / filename
            if not file_path.exists():
                continue
            print(f"   - Traitement batch: {key} ({filename})...")
            results[key] = processor(file_path)
    
    return results


def _build_aggregates(
    api_results: Dict[str, PipelineResult],
    batch_results: Dict[str, PipelineResult],
) -> Dict[str, pd.DataFrame]:
    aggregates: Dict[str, pd.DataFrame] = {}

    # API bikes : donnÃ©es temps rÃ©el VÃ©lib
    bikes_df = api_results.get("bikes").dataframe if api_results.get("bikes") else pd.DataFrame()
    if not bikes_df.empty:
        aggregates["velib_realtime"] = aggregation.aggregate_velib_realtime(bikes_df)

    # API traffic : incidents de circulation
    traffic_df = api_results.get("traffic").dataframe if api_results.get("traffic") else pd.DataFrame()
    if not traffic_df.empty:
        aggregates["traffic_daily"] = aggregation.aggregate_traffic_incidents(traffic_df)

    # API weather : donnÃ©es mÃ©tÃ©o
    weather_df = api_results.get("weather").dataframe if api_results.get("weather") else pd.DataFrame()
    if not weather_df.empty:
        aggregates["weather_daily"] = aggregation.aggregate_weather_daily(weather_df)

    # Batch comptage_velo : comptages historiques de vÃ©los
    comptage_df = batch_results.get("comptage_velo").dataframe if batch_results.get("comptage_velo") else pd.DataFrame()
    if not comptage_df.empty:
        aggregates["comptage_velo_daily"] = aggregation.aggregate_comptage_velo(comptage_df)

    validations_df = batch_results.get("validations").dataframe if batch_results.get("validations") else pd.DataFrame()
    if not validations_df.empty:
        aggregates["validations_daily"] = aggregation.aggregate_validations(validations_df)

    # Construction des KPIs combinÃ©s si on a au moins quelques agrÃ©gats
    if aggregates:
        aggregates["daily_kpis"] = aggregation.build_kpis(aggregates)

    return aggregates


def _calculate_cityflow_metrics(
    api_results: Dict[str, PipelineResult],
    batch_results: Dict[str, PipelineResult],
) -> Dict[str, any]:
    """
    Calcule toutes les mÃ©triques CityFlow Analytics.
    """
    print("\nğŸ”¬ Calcul des mÃ©triques CityFlow Analytics...")
    
    # RÃ©cupÃ©rer les DataFrames nÃ©cessaires
    comptage_df = batch_results.get("comptage_velo").dataframe if batch_results.get("comptage_velo") else pd.DataFrame()
    chantiers_df = batch_results.get("chantiers").dataframe if batch_results.get("chantiers") else pd.DataFrame()
    qualite_df = batch_results.get("qualite_service").dataframe if batch_results.get("qualite_service") else pd.DataFrame()
    geo_df = batch_results.get("referentiel_troncons").dataframe if batch_results.get("referentiel_troncons") else pd.DataFrame()
    
    # Calculer toutes les mÃ©triques
    cityflow_metrics = metrics.calculate_all_metrics(
        df_comptage_velo=comptage_df,
        df_chantiers=chantiers_df,
        df_qualite=qualite_df,
        df_geo=geo_df
    )
    
    print(f"   âœ“ {len(cityflow_metrics)} mÃ©triques CityFlow calculÃ©es")
    return cityflow_metrics


def _calculate_correlations(
    api_results: Dict[str, PipelineResult],
    batch_results: Dict[str, PipelineResult],
) -> Dict[str, pd.DataFrame]:
    """
    Calcule les corrÃ©lations entre les diffÃ©rentes sources de donnÃ©es.
    """
    print("\nğŸ”— Calcul des corrÃ©lations...")
    
    correlations: Dict[str, pd.DataFrame] = {}
    
    # RÃ©cupÃ©rer les DataFrames
    comptage_df = batch_results.get("comptage_velo").dataframe if batch_results.get("comptage_velo") else pd.DataFrame()
    chantiers_df = batch_results.get("chantiers").dataframe if batch_results.get("chantiers") else pd.DataFrame()
    qualite_df = batch_results.get("qualite_service").dataframe if batch_results.get("qualite_service") else pd.DataFrame()
    validations_df = batch_results.get("validations").dataframe if batch_results.get("validations") else pd.DataFrame()
    weather_df = api_results.get("weather").dataframe if api_results.get("weather") else pd.DataFrame()
    
    # CorrÃ©lation chantiers â†” vÃ©lo
    if not comptage_df.empty and not chantiers_df.empty:
        corr_chantiers = aggregation.correlate_chantiers_velo(comptage_df, chantiers_df)
        if not corr_chantiers.empty:
            correlations["chantiers_velo"] = corr_chantiers
    
    # CorrÃ©lation qualitÃ© â†” validations
    if not qualite_df.empty and not validations_df.empty:
        corr_qualite = aggregation.correlate_qualite_validations(qualite_df, validations_df)
        if not corr_qualite.empty:
            correlations["qualite_validations"] = corr_qualite
    
    # CorrÃ©lation mÃ©tÃ©o â†” vÃ©lo
    if not weather_df.empty and not comptage_df.empty:
        corr_meteo = aggregation.correlate_meteo_velo(weather_df, comptage_df)
        if not corr_meteo.empty:
            correlations["meteo_velo"] = corr_meteo
    
    print(f"   âœ“ {len(correlations)} corrÃ©lations calculÃ©es")
    return correlations


def process_day(raw_root: Path, date: str, *, output_root: Path | None = None, writer: OutputWriter | None = None, reader: InputReader | None = None) -> DailyProcessingOutput:
    base_api = raw_root / "raw" / "api"
    base_batch = raw_root / "raw" / "batch" / date

    print("ğŸ“¡ Traitement des sources API...")
    api_results = _process_api_sources(base_api, date, reader)
    print(f"   âœ“ {len(api_results)} sources API traitÃ©es")
    
    print("\nğŸ“¦ Traitement des sources batch...")
    batch_results = _process_batch_sources(base_batch, date, reader)
    print(f"   âœ“ {len(batch_results)} sources batch traitÃ©es")
    
    print("\nğŸ“Š AgrÃ©gation des donnÃ©es...")
    aggregates = _build_aggregates(api_results, batch_results)
    print(f"   âœ“ {len(aggregates)} agrÃ©gats crÃ©Ã©s")

    # NOUVEAU : Calcul des mÃ©triques CityFlow Analytics
    cityflow_metrics = _calculate_cityflow_metrics(api_results, batch_results)
    
    # NOUVEAU : Calcul des corrÃ©lations
    correlations = _calculate_correlations(api_results, batch_results)

    output = DailyProcessingOutput(
        api_results=api_results,
        batch_results=batch_results,
        aggregates=aggregates,
        metrics_cityflow=cityflow_metrics,
        correlations=correlations
    )

    if output_root is not None:
        print(f"\nğŸ’¾ Sauvegarde des rÃ©sultats...")
        _materialise_outputs(output_root, date, output, writer=writer)
        print("   âœ“ AgrÃ©gats sauvegardÃ©s localement")
        if writer is not None:
            print("   âœ“ MÃ©triques, corrÃ©lations et rapports sauvegardÃ©s dans la base de donnÃ©es")

    return output


def _print_summary(output: DailyProcessingOutput, date: str, output_root: Path) -> None:
    print(f"\n{'='*80}")
    print(f"ğŸ“Š RÃ‰SUMÃ‰ DU TRAITEMENT - {date}")
    print(f"{'='*80}")
    
    print("\nğŸ“¡ Sources API traitÃ©es:")
    for source in output.api_results.keys():
        rows = len(output.api_results[source].dataframe)
        print(f"   â€¢ {source}: {rows} enregistrements")
    
    print("\nğŸ“¦ Sources batch traitÃ©es:")
    for source in output.batch_results.keys():
        rows = len(output.batch_results[source].dataframe)
        print(f"   â€¢ {source}: {rows} enregistrements")
    
    print("\nğŸ“Š AgrÃ©gations gÃ©nÃ©rÃ©es:")
    for agg_name in output.aggregates.keys():
        rows = len(output.aggregates[agg_name])
        print(f"   â€¢ {agg_name}: {rows} lignes")
    
    print("\nğŸ”¬ MÃ©triques CityFlow Analytics:")
    for metric_name in output.metrics_cityflow.keys():
        print(f"   â€¢ {metric_name}")
    
    print("\nğŸ”— CorrÃ©lations calculÃ©es:")
    if output.correlations:
        for corr_name in output.correlations.keys():
            rows = len(output.correlations[corr_name])
            print(f"   â€¢ {corr_name}: {rows} lignes")
    else:
        print("   â€¢ Aucune corrÃ©lation calculÃ©e")
    
    print(f"\nğŸ“‚ Fichiers locaux gÃ©nÃ©rÃ©s:")
    print(f"   â€¢ AgrÃ©gats uniquement : {output_root / date / 'metrics'}")
    
    print(f"\nğŸ’¾ DonnÃ©es en base de donnÃ©es:")
    print(f"   â€¢ MÃ©triques CityFlow : collection 'cityflow-metrics'")
    print(f"   â€¢ CorrÃ©lations : collection 'cityflow-daily-correlations'")
    print(f"   â€¢ Rapports (processing, quotidien, summary) : collection 'cityflow-daily-reports'")
    
    print(f"\n{'='*80}")


def _list_available_dates(raw_root: Path) -> List[str]:
    dates: set[str] = set()
    batch_root = raw_root / "raw" / "batch"
    if batch_root.exists():
        dates.update(p.name for p in batch_root.iterdir() if p.is_dir())

    api_root = raw_root / "raw" / "api"
    if api_root.exists():
        for source_dir in api_root.iterdir():
            if not source_dir.is_dir():
                continue
            dates.update(p.name for p in source_dir.iterdir() if p.is_dir())

    return sorted(dates)


def _resolve_processing_date(raw_root: Path, desired: str) -> str:
    available = _list_available_dates(raw_root)
    if desired in available:
        return desired
    if available:
        return available[-1]
    return desired


def _detect_raw_root() -> Path:
    candidates = [
        Path("."),
        Path("bucket-cityflow-paris-s3-raw"),
    ]
    for candidate in candidates:
        if (candidate / "raw").exists():
            return candidate
    return Path(".")


if __name__ == "__main__":
    # Charger la configuration
    cfg = get_config()
    
    print("="*80)
    print("ğŸš€ CITYFLOW ANALYTICS - TRAITEMENT DES DONNÃ‰ES")
    print("="*80)
    print(f"ğŸ“Œ Environnement : {cfg.environment.upper()}")
    print(f"ğŸ“¥ Source Input  : {cfg.input_source}")
    print(f"ğŸ“¤ Cible Output  : {cfg.output_target}")
    print("="*80)
    
    # Initialiser le lecteur et l'Ã©crivain
    reader = InputReader()
    writer = OutputWriter()
    
    print("\nğŸ” DÃ©tection du rÃ©pertoire raw...")
    raw_root = reader.get_raw_root()
    print(f"   âœ“ RÃ©pertoire raw: {raw_root}")
    
    print("\nğŸ“… DÃ©termination de la date de traitement...")
    today = pd.Timestamp.utcnow().strftime("%Y-%m-%d")
    output_root = Path("output")

    target_date = _resolve_processing_date(raw_root, today)
    if target_date != today:
        print(f"   âš  Aucune donnÃ©e pour {today}, traitement de la derniÃ¨re date disponible {target_date}.")
    else:
        print(f"   âœ“ Date de traitement: {target_date}")

    print(f"\nğŸš€ DÃ©marrage du traitement pour {target_date}...\n")
    try:
        output = process_day(raw_root=raw_root, date=target_date, output_root=output_root, writer=writer, reader=reader)
        print("\nâœ… Traitement terminÃ© avec succÃ¨s!")
        _print_summary(output, target_date, output_root)
    except Exception as e:
        print(f"\nâŒ Erreur pendant le traitement: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        # Fermer les connexions
        writer.close()
